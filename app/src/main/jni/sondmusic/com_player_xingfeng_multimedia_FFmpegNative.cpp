/* DO NOT EDIT THIS FILE - it is machine generated */
#include "com_player_xingfeng_multimedia_FFmpegNative.h"

struct URLProtocol;

/*
 * Class:     com_player_xingfeng_multimedia_FFmpegNative
 * Method:    urlprotocolinfo
 * Signature: ()Ljava/lang/String;
 */
JNIEXPORT jstring JNICALL Java_com_player_xingfeng_multimedia_FFmpegNative_urlprotocolinfo
  (JNIEnv *env, jobject obj) {
    char info[40000] = {0};
    av_register_all();

    struct URLProtocol *pup = NULL;
    struct URLProtocol **p_temp = &pup;
    avio_enum_protocols((void **)p_temp, 0);
    while ((*p_temp) !=NULL) {
        sprintf(info, "%s[In ][%10s]\n", info, avio_enum_protocols((void **)p_temp, 0));
    }
    pup = NULL;

    avio_enum_protocols((void **)p_temp, 1);
    while ((*p_temp) !=NULL) {
        sprintf(info, "%s[Out ][%10s]\n", info, avio_enum_protocols((void **)p_temp, 1));
    }

    return env->NewStringUTF(info);
}

/*
 * Class:     com_player_xingfeng_multimedia_FFmpegNative
 * Method:    avformatinfo
 * Signature: ()Ljava/lang/String;
 */
JNIEXPORT jstring JNICALL Java_com_player_xingfeng_multimedia_FFmpegNative_avformatinfo
  (JNIEnv *env, jobject obj) {
    char info[40000] = {0};

    av_register_all();
    AVInputFormat *if_temp = av_iformat_next(NULL);
    AVOutputFormat *of_temp = av_oformat_next(NULL);

    while (if_temp != NULL) {
        sprintf(info, "%s[In ][%10s]\n", info, if_temp->name);
        if_temp=if_temp->next;
    }

    while (of_temp != NULL) {
        sprintf(info, "%s[Out ][%10s]\n", info, of_temp->name);
        of_temp=of_temp->next;
    }

    return env->NewStringUTF(info);
}

/*
 * Class:     com_player_xingfeng_multimedia_FFmpegNative
 * Method:    avcodecinfo
 * Signature: ()Ljava/lang/String;
 */
JNIEXPORT jstring JNICALL Java_com_player_xingfeng_multimedia_FFmpegNative_avcodecinfo
  (JNIEnv *env, jobject obj) {
    char info[40000] = {0};

    av_register_all();

    AVCodec *c_temp = av_codec_next(NULL);

    while (c_temp != NULL) {
        if (c_temp->decode != NULL) {
            sprintf(info, "%s[Dec]", info);
        } else {
            sprintf(info, "%s[Enc]", info);
        }
        switch (c_temp->type) {
            case AVMEDIA_TYPE_VIDEO:
                sprintf(info, "%s[VIDEO]", info);
                break;
            case AVMEDIA_TYPE_AUDIO:
                sprintf(info, "%s[AUDIO]", info);
                break;
            default:
                sprintf(info, "%s[OTHER]", info);
                break;
        }
        sprintf(info, "%s[%10s]\n", info, c_temp->name);
        c_temp=c_temp->next;
    }

    return env->NewStringUTF(info);
}

/*
 * Class:     com_player_xingfeng_multimedia_FFmpegNative
 * Method:    avfilterinfo
 * Signature: ()Ljava/lang/String;
 */
JNIEXPORT jstring JNICALL Java_com_player_xingfeng_multimedia_FFmpegNative_avfilterinfo
  (JNIEnv *env, jobject obj) {
    char info[40000] = { 0 };
    avfilter_register_all();
    AVFilter *f_temp = (AVFilter *)avfilter_next(NULL);
    int len = 0;
    while (f_temp != NULL){
        len = len + sizeof(f_temp->name);
        LOGI(LOG_TAG, "len:" + len);
        sprintf(info, "%s[%10s]\n", info, f_temp->name);
        f_temp=f_temp->next;
    }
    //LOGE("%s", info);

    return env->NewStringUTF(info);
}

/*
 * Class:     com_player_xingfeng_multimedia_FFmpegNative
 * Method:    configurationinfo
 * Signature: ()Ljava/lang/String;
 */
JNIEXPORT jstring JNICALL Java_com_player_xingfeng_multimedia_FFmpegNative_configurationinfo
  (JNIEnv *env, jobject obj) {
    char info[10000] = { 0 };
    av_register_all();

    sprintf(info, "%s\n", avcodec_configuration());

    //LOGE("%s", info);
    return env->NewStringUTF(info);
}


//Output FFmpeg's av_log()
void custom_log(void *ptr, int level, const char* fmt, va_list vl){
    FILE *fp=fopen("/storage/emulated/0/av_log.txt","a+");
    if(fp){
        vfprintf(fp,fmt,vl);
        fflush(fp);
        fclose(fp);
    }
}

JNIEXPORT jint JNICALL
Java_com_player_xingfeng_multimedia_FFmpegNative_decode(JNIEnv *env, jobject instance,
                                                        jstring inputPath, jstring outputPath) {
    const char *inPath = env->GetStringUTFChars(inputPath, NULL);
    const char *outPath = env->GetStringUTFChars(outputPath, NULL);

    // TODO
    env->ReleaseStringUTFChars(inputPath, inPath);
    env->ReleaseStringUTFChars(outputPath, outPath);

    AVFormatContext *pFormatCtx;
    int i, videoindex;
    AVCodecContext *pCodecCtx;
    AVCodec *pCodec;
    AVFrame *pFrame, *pFrameYUV;
    uint8_t *out_buffer;
    AVPacket *packet;
    int y_size;
    int ret, got_picture;
    struct SwsContext *img_convert_ctx;
    FILE *fp_yuv;
    int frame_cnt;
    clock_t time_start, time_finish;
    double time_duration = 0.0;

    char input_str[500] = {0};
    char output_str[500] ={0};
    char info[1000]={0};
    sprintf(input_str,"%s",env->GetStringUTFChars(inputPath, NULL));
    sprintf(output_str,"%s",env->GetStringUTFChars(outputPath, NULL));

    av_log_set_callback(custom_log);

    av_register_all();
    avformat_network_init();
    pFormatCtx = avformat_alloc_context();

    if (avformat_open_input(&pFormatCtx, input_str, NULL,NULL) != 0) {
        LOGE("Couldn't open input stream.\n");
        return -1;
    }

    if (avformat_find_stream_info(pFormatCtx, NULL) < 0) {
        LOGE("Couldn't find stream information.\n");
        return -1;
    }

    videoindex  -1;
    for (i = 0; i< pFormatCtx->nb_streams; i++) {
        if (pFormatCtx->streams[i]->codecpar->codec_type==AVMEDIA_TYPE_VIDEO) {
            videoindex = i;
            break;
        }
    }

    if (videoindex == -1) {
        LOGE("Couldn't find a video stream.\n");
        return  -1;
    }

    pCodec = avcodec_find_decoder(pFormatCtx->streams[videoindex]->codecpar->codec_id);
    pCodecCtx = avcodec_alloc_context3(pCodec);
    avcodec_parameters_to_context(pCodecCtx, pFormatCtx->streams[videoindex]->codecpar);

    if (pCodec == NULL) {
        LOGE("Couldn't find Codec.\n");
        return  -1;
    }
    if (avcodec_open2(pCodecCtx, pCodec, NULL) < 0) {
        LOGE("Couldn't find Codec.\n");
        return  -1;
    }

    pFrame = av_frame_alloc();
    pFrameYUV = av_frame_alloc();
    out_buffer = (unsigned char *)av_malloc(av_image_get_buffer_size(AV_PIX_FMT_YUV420P, pCodecCtx->width, pCodecCtx->height, 1));
    av_image_fill_arrays(pFrameYUV->data, pFrameYUV->linesize, out_buffer,
            AV_PIX_FMT_YUV420P, pCodecCtx->width, pCodecCtx->height, 1);

    packet = (AVPacket *)av_malloc(sizeof(AVPacket));

    img_convert_ctx = sws_getContext(pCodecCtx->width, pCodecCtx->height, pCodecCtx->pix_fmt,
            pCodecCtx->width, pCodecCtx->height, AV_PIX_FMT_YUV420P,  SWS_BICUBIC, NULL, NULL, NULL);

    sprintf(info,   "[Input     ]%s\n", input_str);
    sprintf(info, "%s[Output    ]%s\n",info,output_str);
    sprintf(info, "%s[Format    ]%s\n",info, pFormatCtx->iformat->name);
    sprintf(info, "%s[Codec     ]%s\n",info, pCodecCtx->codec_name);
    sprintf(info, "%s[Resolution]%dx%d\n",info, pCodecCtx->width,pCodecCtx->height);

    fp_yuv=fopen(output_str,"wb+");

    if(fp_yuv==NULL){
        printf("Cannot open output file.\n");
        return -1;
    }

    frame_cnt=0;
    time_start = clock();

    while(av_read_frame(pFormatCtx, packet)>=0){
        if(packet->stream_index==videoindex){
            ret = avcodec_decode_video2(pCodecCtx, pFrame, &got_picture, packet);
            if(ret < 0){
                LOGE("Decode Error.\n");
                return -1;
            }
            if(got_picture){
                sws_scale(img_convert_ctx, (const uint8_t* const*)pFrame->data, pFrame->linesize, 0, pCodecCtx->height,
                          pFrameYUV->data, pFrameYUV->linesize);

                y_size=pCodecCtx->width*pCodecCtx->height;
                fwrite(pFrameYUV->data[0],1,y_size,fp_yuv);    //Y
                fwrite(pFrameYUV->data[1],1,y_size/4,fp_yuv);  //U
                fwrite(pFrameYUV->data[2],1,y_size/4,fp_yuv);  //V
                //Output info
                char pictype_str[10]={0};
                switch(pFrame->pict_type){
                    case AV_PICTURE_TYPE_I:sprintf(pictype_str,"I");break;
                    case AV_PICTURE_TYPE_P:sprintf(pictype_str,"P");break;
                    case AV_PICTURE_TYPE_B:sprintf(pictype_str,"B");break;
                    default:sprintf(pictype_str,"Other");break;
                }
                LOGI("Frame Index: %5d. Type:%s",frame_cnt,pictype_str);
                frame_cnt++;
            }
        }
        av_free_packet(packet);
    }


    //flush decoder
    //FIX: Flush Frames remained in Codec
    while (1) {
        ret = avcodec_decode_video2(pCodecCtx, pFrame, &got_picture, packet);
        if (ret < 0)
            break;
        if (!got_picture)
            break;
        sws_scale(img_convert_ctx, (const uint8_t* const*)pFrame->data, pFrame->linesize, 0, pCodecCtx->height,
                  pFrameYUV->data, pFrameYUV->linesize);
        int y_size=pCodecCtx->width*pCodecCtx->height;
        fwrite(pFrameYUV->data[0],1,y_size,fp_yuv);    //Y
        fwrite(pFrameYUV->data[1],1,y_size/4,fp_yuv);  //U
        fwrite(pFrameYUV->data[2],1,y_size/4,fp_yuv);  //V
        //Output info
        char pictype_str[10]={0};
        switch(pFrame->pict_type){
            case AV_PICTURE_TYPE_I:sprintf(pictype_str,"I");break;
            case AV_PICTURE_TYPE_P:sprintf(pictype_str,"P");break;
            case AV_PICTURE_TYPE_B:sprintf(pictype_str,"B");break;
            default:sprintf(pictype_str,"Other");break;
        }
        LOGI("Frame Index: %5d. Type:%s",frame_cnt,pictype_str);
        frame_cnt++;
    }
    time_finish = clock();
    time_duration=(double)(time_finish - time_start);

    sprintf(info, "%s[Time      ]%fms\n",info,time_duration);
    sprintf(info, "%s[Count     ]%d\n",info,frame_cnt);

    sws_freeContext(img_convert_ctx);

    fclose(fp_yuv);

    av_frame_free(&pFrameYUV);
    av_frame_free(&pFrame);
    avcodec_close(pCodecCtx);
    avformat_close_input(&pFormatCtx);

    return 0;
}

/**
 * 只支持flv本地文件的推流,经测试没有问题
 * @param env
 * @param obj
 * @param fileName_
 * @param pushUrl_
 * @return
 */
JNIEXPORT jint JNICALL
Java_com_player_xingfeng_multimedia_FFmpegNative_pushStream(JNIEnv *env, jobject obj,
                                                             jstring fileName_, jstring pushUrl_) {
    //所有代码执行之前要调用av_register_all和avformat_network_init
    //初始化所有的封装和解封装 flv mp4 mp3 mov。不包含编码和解码
    av_register_all();

    //初始化网络库
    avformat_network_init();

    int ret = 0;
    //封装上下文
    AVFormatContext* ictx = NULL;
    AVFormatContext* octx = NULL;
    const char* iurl = env->GetStringUTFChars(fileName_, false);
    const char* ourl = env->GetStringUTFChars(pushUrl_, false);

    LOGD("rtmp://192.168.43.24/rtmplive/room");
    //打开文件，解封文件头
    ret = avformat_open_input(&ictx, iurl, NULL, NULL);
    if (ret != 0) {
        LOGD("avformat_open_input fail");
        return ret;
    }

    //获取音视频流信息,h264 flv
    ret = avformat_find_stream_info(ictx, NULL);
    if (ret != 0) {
        LOGD("avformat_find_stream_info fail");
        return ret;
    }

    //打印媒体信息
    av_dump_format(ictx, 0, iurl, 0);

    //////////////////////////////

    //输出流
    ret = avformat_alloc_output_context2(&octx, NULL, "flv", ourl);
    if (ret != 0) {
        LOGD("avformat_alloc_output_context2 fail");
        return ret;
    }

    LOGD("avformat_alloc_output_context2 success");

    //配置输出流
    for (int i = 0; i < ictx->nb_streams; ++i)
    {
        //创建流
        AVStream* ostream = avformat_new_stream(octx, avcodec_find_encoder(ictx->streams[i]->codecpar->codec_id));
        if (ostream == NULL)
            return -1;
        //复制配置信息
        ret = avcodec_parameters_copy(ostream->codecpar, ictx->streams[i]->codecpar);
        if (ret != 0)
            return ret;
        ostream->codecpar->codec_tag = 0;//标记不需要重新编解码
    }
    av_dump_format(octx, 0, ourl, 1);

    //////////////////////////////

    ret = avio_open(&octx->pb, ourl, AVIO_FLAG_WRITE);
    if (ret < 0) {
        LOGD("avio_open fail");
        return ret;
    } else {
        LOGD("avio_open success");
    }

    //写入头信息
    ret = avformat_write_header(octx, NULL);
    if (ret < 0) {
        LOGD("avformat_write_header fail");
        return ret;
    } else {
        LOGD("avformat_write_header success");
    }

    //推流每一帧数据
    AVPacket pkt;
    int64_t starttime = av_gettime();
    while (av_read_frame(ictx, &pkt) == 0)
    {
        //计算转换pts dts
        AVRational itime = ictx->streams[pkt.stream_index]->time_base;
        AVRational otime = octx->streams[pkt.stream_index]->time_base;
        pkt.pts = av_rescale_q_rnd(pkt.pts, itime, otime, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));
        pkt.dts = av_rescale_q_rnd(pkt.dts, itime, otime, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));
        pkt.duration = av_rescale_q_rnd(pkt.duration, itime, otime, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));
        pkt.pos = -1;

        if (ictx->streams[pkt.stream_index]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO)
        {
            int64_t nowtime = av_gettime() - starttime;
            int64_t dts = pkt.dts * av_q2d(octx->streams[pkt.stream_index]->time_base) * 1000 * 1000;
            if(dts > nowtime)
                av_usleep(dts- nowtime);
        }

        ret = av_interleaved_write_frame(octx, &pkt);
        LOGD("av_interleaved_write_frame success");
        av_packet_unref(&pkt);
        if (ret < 0) {
            LOGD("av_interleaved_write_frame fail");
            return ret;
        } else {
            LOGD("av_interleaved_write_frame success");
        }
    }

    env->ReleaseStringUTFChars(fileName_, iurl);
    env->ReleaseStringUTFChars(pushUrl_, ourl);
    return 0;
}

/**
 * 据说可以支持flv及mp4本地文件的推流,经测试flv没有问题,mp4还是🈶问题。待有时间再研究
 * @param env
 * @param obj
 * @param fileName_
 * @param pushUrl_
 * @return
 */
JNIEXPORT jint JNICALL
Java_com_player_xingfeng_multimedia_FFmpegNative_pushStream2(JNIEnv *env, jobject obj,
                                                            jstring fileName_, jstring pushUrl_) {
    int videoindex = -1;
    //所有代码执行之前要调用av_register_all和avformat_network_init
    //初始化所有的封装和解封装 flv mp4 mp3 mov。不包含编码和解码
    av_register_all();

    //初始化网络库
    avformat_network_init();

    //使用的相对路径，执行文件在bin目录下。test.mp4放到bin目录下即可
    const char *inUrl = env->GetStringUTFChars(fileName_, false);
    //输出的地址
    const char *outUrl = env->GetStringUTFChars(pushUrl_, false);

    //////////////////////////////////////////////////////////////////
    //                   输入流处理部分
    /////////////////////////////////////////////////////////////////
    //打开文件，解封装 avformat_open_input
    //AVFormatContext **ps  输入封装的上下文。包含所有的格式内容和所有的IO。如果是文件就是文件IO，网络就对应网络IO
    //const char *url  路径
    //AVInputFormt * fmt 封装器
    //AVDictionary ** options 参数设置
    AVFormatContext *ictx = NULL;

    AVOutputFormat *ofmt = NULL;

    //打开文件，解封文件头
    int ret = avformat_open_input(&ictx, inUrl, 0, NULL);
    if (ret < 0) {
        return avError(ret);
    }
    LOGD("avformat_open_input success!");
    //获取音频视频的信息 .h264 flv 没有头信息
    ret = avformat_find_stream_info(ictx, 0);
    if (ret != 0) {
        return avError(ret);
    }
    //打印视频视频信息
    //0打印所有  inUrl 打印时候显示，
    av_dump_format(ictx, 0, inUrl, 0);

    //////////////////////////////////////////////////////////////////
    //                   输出流处理部分
    /////////////////////////////////////////////////////////////////
    AVFormatContext * octx = NULL;
    //如果是输入文件 flv可以不传，可以从文件中判断。如果是流则必须传
    //创建输出上下文
    ret = avformat_alloc_output_context2(&octx, NULL, "flv", outUrl);
    if (ret < 0) {
        return avError(ret);
    }
    LOGD("avformat_alloc_output_context2 success!");

    ofmt = octx->oformat;
    //cout << "nb_streams  " << ictx->nb_streams << endl;
    int i;
    //for (i = 0; i < ictx->nb_streams; i++) {
    //  cout << "i " << i <<"  "<< ictx->nb_streams<< endl;
    //  AVStream *in_stream = ictx->streams[i];
    //  AVCodec *codec = avcodec_find_decoder(in_stream->codecpar->codec_id);
    //  AVStream *out_stream = avformat_new_stream(octx, codec);
    //  if (!out_stream) {
    //      printf("Failed allocating output stream\n");
    //      ret = AVERROR_UNKNOWN;
    //  }
    //  AVCodecContext *pCodecCtx = avcodec_alloc_context3(codec);
    //  ret = avcodec_parameters_to_context(pCodecCtx, in_stream->codecpar);
    //  if (ret < 0) {
    //      printf("Failed to copy context input to output stream codec context\n");
    //  }
    //  pCodecCtx->codec_tag = 0;
    //  if (octx->oformat->flags & AVFMT_GLOBALHEADER) {
    //      pCodecCtx->flags |= CODEC_FLAG_GLOBAL_HEADER;
    //  }
    //  ret = avcodec_parameters_from_context(out_stream->codecpar, pCodecCtx);
    //  if (ret < 0) {
    //      printf("Failed to copy context input to output stream codec context\n");
    //  }
    //}

    for (i = 0; i < ictx->nb_streams; i++) {

        //获取输入视频流
        AVStream *in_stream = ictx->streams[i];
        //为输出上下文添加音视频流（初始化一个音视频流容器）
        AVStream *out_stream = avformat_new_stream(octx, avcodec_find_encoder(in_stream->codecpar->codec_id));
        if (!out_stream) {
            printf("未能成功添加音视频流\n");
            ret = AVERROR_UNKNOWN;
        }


        AVCodecContext *codec_ctx = avcodec_alloc_context3(avcodec_find_encoder(in_stream->codecpar->codec_id));
        ret = avcodec_parameters_to_context(codec_ctx, in_stream->codecpar);
        if (ret < 0){
            printf("Failed to copy in_stream codecpar to codec context\n");
            //goto end;
        }

        codec_ctx->codec_tag = 0;
        if (octx->oformat->flags & AVFMT_GLOBALHEADER)
            codec_ctx->flags |= AV_CODEC_FLAG_GLOBAL_HEADER;

        ret = avcodec_parameters_from_context(out_stream->codecpar, codec_ctx);
        if (ret < 0) {
            LOGD("copy 编解码器上下文失败");
        }

//        //将输入编解码器上下文信息 copy 给输出编解码器上下文
//        //ret = avcodec_copy_context(out_stream->codec, in_stream->codec);
//        ret = avcodec_parameters_copy(out_stream->codecpar, in_stream->codecpar);
//        //ret = avcodec_parameters_from_context(out_stream->codecpar, in_stream->codec);
//        //ret = avcodec_parameters_to_context(out_stream->codec, in_stream->codecpar);
//        if (ret < 0) {
//            printf("copy 编解码器上下文失败\n");
//        }
//
//        out_stream->codecpar->codec_tag = 0;
//        if (octx->oformat->flags & AVFMT_GLOBALHEADER) {
//            out_stream->codecpar->flags = out_stream->codec->flags | CODEC_FLAG_GLOBAL_HEADER;
//        }
    }

    //输入流数据的数量循环
    for (i = 0; i < ictx->nb_streams; i++) {
        if (ictx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO) {
            videoindex = i;
            break;
        }
    }

    av_dump_format(octx, 0, outUrl, 1);

    //////////////////////////////////////////////////////////////////
    //                   准备推流
    /////////////////////////////////////////////////////////////////

    //打开IO
    ret = avio_open(&octx->pb, outUrl, AVIO_FLAG_WRITE);
    if (ret < 0) {
        avError(ret);
    }

    //写入头部信息
    ret = avformat_write_header(octx, 0);
    if (ret < 0) {
        avError(ret);
    }
    LOGD("avformat_write_header Success!");
    //推流每一帧数据
    //int64_t pts  [ pts*(num/den)  第几秒显示]
    //int64_t dts  解码时间 [P帧(相对于上一帧的变化) I帧(关键帧，完整的数据) B帧(上一帧和下一帧的变化)]  有了B帧压缩率更高。
    //uint8_t *data
    //int size
    //int stream_index
    //int flag
    AVPacket pkt;
    //获取当前的时间戳  微妙
    long long start_time = av_gettime();
    long long frame_index = 0;
    while (1) {
        //输入输出视频流
        AVStream *in_stream, *out_stream;
        //获取解码前数据
        ret = av_read_frame(ictx, &pkt);
        if (ret < 0) {
            break;
        }

        /*
        PTS（Presentation Time Stamp）显示播放时间
        DTS（Decoding Time Stamp）解码时间
        */
        //没有显示时间（比如未解码的 H.264 ）
        if (pkt.pts == AV_NOPTS_VALUE) {
            //AVRational time_base：时基。通过该值可以把PTS，DTS转化为真正的时间。
            AVRational time_base1 = ictx->streams[videoindex]->time_base;

            //计算两帧之间的时间
            /*
            r_frame_rate 基流帧速率  （不是太懂）
            av_q2d 转化为double类型
            */
            int64_t calc_duration = (double)AV_TIME_BASE / av_q2d(ictx->streams[videoindex]->r_frame_rate);

            //配置参数
            pkt.pts = (double)(frame_index*calc_duration) / (double)(av_q2d(time_base1)*AV_TIME_BASE);
            pkt.dts = pkt.pts;
            pkt.duration = (double)calc_duration / (double)(av_q2d(time_base1)*AV_TIME_BASE);
        }

        //延时
        if (pkt.stream_index == videoindex) {
            AVRational time_base = ictx->streams[videoindex]->time_base;
            AVRational time_base_q = { 1,AV_TIME_BASE };
            //计算视频播放时间
            int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q);
            //计算实际视频的播放时间
            int64_t now_time = av_gettime() - start_time;

            AVRational avr = ictx->streams[videoindex]->time_base;
            //cout << avr.num << " " << avr.den << "  "<<pkt.dts <<"  "<<pkt.pts<<"   "<< pts_time <<endl;
            if (pts_time > now_time) {
                //睡眠一段时间（目的是让当前视频记录的播放时间与实际时间同步）
                av_usleep((unsigned int)(pts_time - now_time));
            }
        }

        in_stream = ictx->streams[pkt.stream_index];
        out_stream = octx->streams[pkt.stream_index];

        //计算延时后，重新指定时间戳
        pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base,(AVRounding) (AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));
        pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));
        pkt.duration = (int)av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);
        //字节流的位置，-1 表示不知道字节流位置
        pkt.pos = -1;

        if (pkt.stream_index == videoindex) {
            LOGD("Send %8d video frames to output URL\n", frame_index);
            frame_index++;
        }

        //向输出上下文发送（向地址推送）
        ret = av_interleaved_write_frame(octx, &pkt);

        if (ret < 0) {
            printf("发送数据包出错\n");
            break;
        }

        //释放
        av_free_packet(&pkt);
    }
    return 0;
}

const char *filters_descr = "drawbox=x=100:y=100:w=100:h=100:color=pink@0.5";

int getCodecContext(AVCodecParameters *codecpar, AVCodecContext **avCodecContext) {
    AVCodec *dec = avcodec_find_decoder(codecpar->codec_id);
    if (!dec) {
        LOGD("can not find decoder");
        return -1;
    }

    *avCodecContext = avcodec_alloc_context3(dec);
    if (!*avCodecContext) {
        LOGD("can not alloc new decodecctx");
        return -1;
    }

    if (avcodec_parameters_to_context(*avCodecContext, codecpar) < 0) {
        avcodec_free_context(avCodecContext);
        *avCodecContext = NULL;
        LOGD("can not fill decodecctx");
        return -1;
    }

    if (avcodec_open2(*avCodecContext, dec, 0) != 0) {
        LOGD("cant not open audio strames");
        avcodec_free_context(avCodecContext);
        *avCodecContext = NULL;
        return -1;
    }
    return 0;
}

/**
 *
 * @param env
 * @param instance
 * @param url
 * @param surface
 * @return
 */
JNIEXPORT void JNICALL
Java_com_player_xingfeng_multimedia_FFmpegNative_play(JNIEnv *env, jobject instance,
        jstring url_, jobject surface) {
    const char *url = env->GetStringUTFChars(url_, 0);

    AVFormatContext *pFormatCtx = NULL;
    AVCodecContext *pCodecCtx = NULL;

    AVFilterContext *buffersink_ctx = NULL;
    AVFilterContext *buffersrc_ctx = NULL;
    AVFilterGraph *filter_graph = NULL;

    //-----------------------------AVCodecContext init start----------------------------
    av_register_all();
    avfilter_register_all();//
    pFormatCtx = avformat_alloc_context();

    // Open video file
    if (avformat_open_input(&pFormatCtx, url, NULL, NULL) != 0) {

        LOGD("Couldn't open url:%s\n", url);
        return; // Couldn't open file
    }

    // Retrieve stream information
    if (avformat_find_stream_info(pFormatCtx, NULL) < 0) {
        LOGD("Couldn't find stream information.");
        return;
    }

    // Find the first video stream
    int videoStream = -1, i;
    for (i = 0; i < pFormatCtx->nb_streams; i++) {
        if (pFormatCtx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO
            && videoStream < 0) {
            videoStream = i;
        }
    }
    if (videoStream == -1) {
        LOGD("Didn't find a video stream.");
        return; // Didn't find a video stream
    }

    if (getCodecContext(pFormatCtx->streams[videoStream]->codecpar, &pCodecCtx) != 0) {
        LOGD("Didn't get CodecContext.");
        return;
    }
    //-----------------------------AVCodecContext init end-------------------------------

    //------------------------------filter init start------------------------------------
    char args[512];
    AVFilter *buffersrc = avfilter_get_by_name("buffer");
    AVFilter *buffersink = avfilter_get_by_name("buffersink");//新版的ffmpeg库必须为buffersink
    AVFilterInOut *outputs = avfilter_inout_alloc();
    AVFilterInOut *inputs = avfilter_inout_alloc();

    enum AVPixelFormat pix_fmts[] = {AV_PIX_FMT_YUV420P, AV_PIX_FMT_NONE};

    filter_graph = avfilter_graph_alloc();

    /* buffer video source: the decoded frames from the decoder will be inserted here. */
    snprintf(args, sizeof(args),
             "video_size=%dx%d:pix_fmt=%d:time_base=%d/%d:pixel_aspect=%d/%d",
             pCodecCtx->width, pCodecCtx->height, pCodecCtx->pix_fmt,
             pFormatCtx->streams[videoStream]->time_base.num,
             pFormatCtx->streams[videoStream]->time_base.den,
             pCodecCtx->sample_aspect_ratio.num, pCodecCtx->sample_aspect_ratio.den);

    if (avfilter_graph_create_filter(&buffersrc_ctx, buffersrc, "in",
                                     args, NULL, filter_graph) < 0) {
        LOGD("Cannot create buffer source\n");
        return;
    }
    AVBufferSinkParams *buffersink_params = av_buffersink_params_alloc();
    buffersink_params->pixel_fmts = pix_fmts;
    if (avfilter_graph_create_filter(&buffersink_ctx, buffersink, "out",
                                     NULL, buffersink_params, filter_graph) < 0) {
        LOGD("Cannot create buffer sink\n");
        return;
    }
    av_free(buffersink_params);

    /* Endpoints for the filter graph. */
    outputs->name = av_strdup("in");
    outputs->filter_ctx = buffersrc_ctx;
    outputs->pad_idx = 0;
    outputs->next = NULL;

    inputs->name = av_strdup("out");
    inputs->filter_ctx = buffersink_ctx;
    inputs->pad_idx = 0;
    inputs->next = NULL;

    if ((avfilter_graph_parse_ptr(filter_graph, filters_descr,
                                  &inputs, &outputs, NULL)) < 0) {
        LOGD("Cannot avfilter_graph_parse_ptr\n");
        return;
    }

    if ((avfilter_graph_config(filter_graph, NULL)) < 0) {
        LOGD("Cannot avfilter_graph_config\n");
        return;
    }
    //------------------------------filter init end------------------------------------

    //------------------------------window init start-----------------------------------
    // 获取native window
    ANativeWindow *nativeWindow = ANativeWindow_fromSurface(env, surface);

    // 获取视频宽高
    int videoWidth = pCodecCtx->width;
    int videoHeight = pCodecCtx->height;

    // 设置native window的buffer大小,可自动拉伸
    ANativeWindow_setBuffersGeometry(nativeWindow, videoWidth, videoHeight,
                                     WINDOW_FORMAT_RGBA_8888);
    ANativeWindow_Buffer windowBuffer;
    //------------------------------window init end-----------------------------------


    //------------------------------get data-----------------------------------


    // 用于渲染
    AVFrame *pFrameRGBA = av_frame_alloc();

    // Determine required buffer size and allocate buffer
    // buffer中数据就是用于渲染的,且格式为RGBA
    int numBytes = av_image_get_buffer_size(AV_PIX_FMT_RGBA, pCodecCtx->width, pCodecCtx->height,
                                            1);
    uint8_t *buffer = (uint8_t *) av_malloc(numBytes * sizeof(uint8_t));
    av_image_fill_arrays(pFrameRGBA->data, pFrameRGBA->linesize, buffer, AV_PIX_FMT_RGBA,
                         pCodecCtx->width, pCodecCtx->height, 1);

    // 由于解码出来的帧格式不是RGBA的,在渲染之前需要进行格式转换
    SwsContext *sws_ctx = sws_getContext(pCodecCtx->width,
                                         pCodecCtx->height,
                                         pCodecCtx->pix_fmt,
                                         pCodecCtx->width,
                                         pCodecCtx->height,
                                         AV_PIX_FMT_RGBA,
                                         SWS_BILINEAR,
                                         NULL,
                                         NULL,
                                         NULL);

    AVPacket *packet = av_packet_alloc();
    int count = 0;
    while (av_read_frame(pFormatCtx, packet) == 0) {
        // Is this a packet from the video stream?
        if (packet->stream_index == videoStream) {
            // Decode video frame
            if (avcodec_send_packet(pCodecCtx, packet) != 0) {
                break;
            }

            AVFrame *pFrame = av_frame_alloc();

            while (avcodec_receive_frame(pCodecCtx, pFrame) == 0) {
                // lock native window buffer
                ANativeWindow_lock(nativeWindow, &windowBuffer, 0);

                //for AVfilter start
                pFrame->pts = av_frame_get_best_effort_timestamp(pFrame);
                //* push the decoded frame into the filtergraph
                if (av_buffersrc_add_frame(buffersrc_ctx, pFrame) == 0) {
                    av_buffersink_get_frame(buffersink_ctx, pFrame);
                } else{
                    LOGD("Could not av_buffersrc_add_frame");
                }
                // 格式转换
                sws_scale(sws_ctx, (uint8_t const *const *) pFrame->data,
                          pFrame->linesize, 0, pCodecCtx->height,
                          pFrameRGBA->data, pFrameRGBA->linesize);

                // 获取stride
                uint8_t *dst = (uint8_t *) windowBuffer.bits;
                int dstStride = windowBuffer.stride * 4;
                uint8_t *src = (pFrameRGBA->data[0]);
                int srcStride = pFrameRGBA->linesize[0];

                // 由于window的stride和帧的stride不同,因此需要逐行复制
                int h;
                for (h = 0; h < videoHeight; h++) {
                    memcpy(dst + h * dstStride, src + h * srcStride, srcStride);
                }
                ANativeWindow_unlockAndPost(nativeWindow);

                count++;
                LOGD("解码渲染%d帧", count);
            }
            av_frame_free(&pFrame);
            av_free(pFrame);
        }
    }


    if (nativeWindow) {
       ANativeWindow_release(nativeWindow);
    }
    av_packet_free(&packet);
    sws_freeContext(sws_ctx);

    avfilter_inout_free(&outputs);
    av_free(outputs);
    avfilter_inout_free(&inputs);
    av_free(inputs);

    av_free(buffer);
    av_frame_free(&pFrameRGBA);
    av_free(pFrameRGBA);

    avfilter_graph_free(&filter_graph); //for avfilter
    // Close the codecs
    avcodec_close(pCodecCtx);
    avcodec_free_context(&pCodecCtx);
    pCodecCtx = NULL;

    // Close the video file
    avformat_close_input(&pFormatCtx);
    avformat_free_context(pFormatCtx);
    pFormatCtx = NULL;

    env->ReleaseStringUTFChars(url_, url);
}